{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "This Notebook takes raw CARLA data in `../data/00_data_raw` and, after a series of intermediary steps, produce `../data/03_preprocessed/01_data.csv`. This csv file can be easily used by the next Notebook, `01_Process-Data.ipynb`, to prepare data for Trajectron++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PATH = '../data/00_data_raw'\n",
    "RAW_CSV_PATH = '../data/01_data_raw_csv'\n",
    "RAW_CSV_TEMPLATE = '../data/01_data_raw_csv/%04d.csv'\n",
    "CSV_PATH = '../data/02_data_csv'\n",
    "CSV_TEMPLATE = '../data/02_data_csv/%04d.csv'\n",
    "CSV_MERGED = '../data/03_preprocessed/00_merged.csv'\n",
    "OUTFILE = '../data/03_preprocessed/01_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path):\n",
    "    for P,D,F in os.walk(path):\n",
    "        F = [f for f in F if os.path.splitext(f)[1] in ['.txt', '.csv']]\n",
    "        return sorted(os.path.join(P,f) for f in F)\n",
    "\n",
    "def get_node_id(filename):\n",
    "    return int(os.path.splitext(os.path.basename(filename))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Functions\n",
    "Individual functions each representing a step in the data pre-processing chain. Takes raw output data from CARLA and pre-processes it for easy digestion into the Trajectron++ system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes CARLA output file in 00_data_raw and turns it into CSVs\n",
    "# in 01_data_raw_csv\n",
    "def create_raw_csv(filename):\n",
    "    with open(filename) as READ:\n",
    "        data = READ.read()\n",
    "        \n",
    "    data_csv = data.replace(' | ',',').replace('Position', 'Pos_X,Pos_Y,Pos_Z') \\\n",
    "                                      .replace('Angular Velocity', 'AVel_1,AVel_2,AVel_3') \\\n",
    "                                      .replace('Velocity', 'Vel_X,Vel_Y,Vel_Z') \\\n",
    "                                      .replace('Acceleration', 'Accel_X,Accel_Y,Accel_Z') \\\n",
    "                                      .replace('Stopped at Red Light + Light ID ', 'LightStop,LightID')\n",
    "    \n",
    "    node_id = int(filename.split('myrecording')[1].split('.')[0])\n",
    "    outfile = RAW_CSV_TEMPLATE % node_id\n",
    "\n",
    "    with open(outfile, 'w') as WRITE:\n",
    "        WRITE.write(data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes CSV file in 01_data_raw_csv, drops unused data and converts\n",
    "# data into correct coordinate system for Trajectron++. Writes converted\n",
    "# CSV file to 02_data_csv\n",
    "def create_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # drop unneeded columns\n",
    "    df = df.drop('Timestamp',1).drop('AVel_1',1).drop('AVel_2',1) \\\n",
    "           .drop('AVel_3',1).drop('LightStop',1).drop('LightID',1)\n",
    "\n",
    "    # add NodeID column with number from filename\n",
    "    node_id = get_node_id(filename)\n",
    "    df['NodeID'] = node_id\n",
    "    \n",
    "    # Convert Simulation Frame into Sample Frame\n",
    "    df['Frame'] = df['Simulation Frame'].apply(lambda x: x//10)\n",
    "    df = df.drop('Simulation Frame',1)\n",
    "    \n",
    "    # Convert Heading to Radian with 0 radian along x+ axis, pi/2 along y+ axis\n",
    "    df['Heading'] = df['Heading'].apply(lambda x: -1 * (x-90) * np.pi / 180)\n",
    "\n",
    "    # Ordering for Columns\n",
    "    column_ordering = ['Frame',\n",
    "                       'NodeID',\n",
    "                       'Pos_X',\n",
    "                       'Pos_Y',\n",
    "                       'Pos_Z',\n",
    "                       'Vel_X',\n",
    "                       'Vel_Y',\n",
    "                       'Vel_Z',\n",
    "                       'Accel_X',\n",
    "                       'Accel_Y',\n",
    "                       'Accel_Z',\n",
    "                       'Heading']\n",
    "\n",
    "    df = df[column_ordering]\n",
    "    \n",
    "    outfile = CSV_TEMPLATE % node_id\n",
    "    \n",
    "    with open(outfile,'w') as WRITE:\n",
    "        WRITE.write(df.to_csv(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes CSV files in 02_data_csv and merges them into one\n",
    "# large CSV called merged.csv\n",
    "def create_merged_csv():\n",
    "    dfs = {}\n",
    "    for f in get_files(CSV_PATH):\n",
    "        dfs[get_node_id(f)] = pd.read_csv(f)\n",
    "    \n",
    "    df_all = pd.DataFrame()\n",
    "    for df in dfs.values():\n",
    "        df_all = df_all.append(df)\n",
    "    \n",
    "    with open(CSV_MERGED, 'w') as WRITE:\n",
    "        WRITE.write(df_all.to_csv(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes merged.csv and enforces that a unique node ID is tracked\n",
    "# for one concurrent peroid of time. Writes result to the\n",
    "# final data.csv\n",
    "# Trajectron++ requires all data for a tracked object to be\n",
    "# consecutive. Thus, if an object enters and leaves tracking,\n",
    "# it must be assigned a new Node ID so all observed timesteps\n",
    "# remain consecutive.\n",
    "def create_consecutive_csv():\n",
    "    df = pd.read_csv(CSV_MERGED)\n",
    "    df.sort_values(['NodeID','Frame'], inplace=True)\n",
    "\n",
    "    node_id = 0\n",
    "    # we need keep cycling until all nodes are consecutive\n",
    "    while node_id <= max(pd.unique(df['NodeID'])):\n",
    "    #for node_id in pd.unique(df['NodeID']):\n",
    "        node_df = df[df['NodeID'] == node_id]\n",
    "\n",
    "        diff = np.diff(node_df['Frame'])\n",
    "        if np.all(diff == 1):\n",
    "            node_id += 1\n",
    "            continue\n",
    "\n",
    "        splits = np.where(diff != 1)[0]\n",
    "        split = splits[0]\n",
    "        #for split in splits:\n",
    "        \n",
    "        # get the last value in Frame in current run\n",
    "        split_frame = node_df.iloc[split]['Frame']\n",
    "\n",
    "        # Get the indices that need to be updated\n",
    "        split_indices = df[(df['NodeID'] == node_id) & (df['Frame'] > split_frame)].index\n",
    "\n",
    "        # determine an available NodeID number\n",
    "        new_id = max(pd.unique(df['NodeID'])) + 1\n",
    "\n",
    "        # assign NodeID number\n",
    "        df.loc[split_indices, 'NodeID'] = new_id\n",
    "        \n",
    "        node_id += 1\n",
    "\n",
    "    with open(OUTFILE, 'w') as WRITE:\n",
    "        WRITE.write(df.to_csv(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute\n",
    "Runs the data processing steps in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in get_files(RAW_PATH):\n",
    "    create_raw_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in get_files(RAW_CSV_PATH):\n",
    "    create_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_merged_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_consecutive_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (Trajectron++)",
   "language": "python",
   "name": "trajectronpp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
